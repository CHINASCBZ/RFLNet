#this code is based on BASNet for RFLNet: Reverse Feature Learning Network for Salient Object Detection in Forward-Looking Sonar Images
# if you have any questions please contact with the author HFL,email ：HFL_CHN@163.com. wechat：H2363050138




import torch   进口火炬   进口火炬
import torch.nn as nn   进口火炬。Nn as Nn
from torchvision import models从火炬视觉导入模型
import torch.nn.functional as F进口火炬。n.功能为F
from torchvision.models import ResNet34_Weights从torchvision。models导入ResNet34_Weights
from model.resnet_model import *
from torch.nn import functional从火炬。Nn导入函数

class channelmix(nn.Module):   #将中间输出的图片，经过rcl卷积，将mask进行输入，在进行bhs模型损失
    def __init__(self,inchannel,outchannel,scale=4):def __init__(自我、inchannel outchannel、规模= 4):
        super(channelmix,self).__init__()超级(channelmix自我). __init__ ()
        self.scale_channel = int(inchannel/scale)自我。Scale_channel = int（inchannel/scale）
        #----------------------------------------上下channel公共卷积------------------------------------------------------#
        self.conv3x3 = nn.Conv2d(in_channels=self.scale_channel,out_channels=self.scale_channel,kernel_size=3,stride=1,padding=1)自我。conv3x3 = nn.Conv2d（in_channels=self.scale_channel,out_channels=self.scale_channel,kernel_size=3,stride=1,padding=1）
        self.batchnorm = nn.BatchNorm2d(self.scale_channel)自我。batchnorm = nn.BatchNorm2d（self.scale_channel）
        self.relu = nn.ReLU(inplace=True)自我。relu = nn.ReLU(inplace=True)

        #------------------------上面channel卷积---------------------------------#
        self.conv3x3_up_1 = nn.Conv2d(in_channels=inchannel,out_channels=outchannel,kernel_size=3,stride=1,padding=1)自我。Conv3x3_up_1 = nn。Conv2d (in_channels = inchannel out_channels = outchannel kernel_size = 3,跨步= 1,填充= 1)
        self.batchnorm_up_1 = nn.BatchNorm2d(outchannel)自我。Batchnorm_up_1 = nn。BatchNorm2d (outchannel)
        self.relu_up_1 = nn.ReLU(inplace=True)自我。relu_up_1 = nn。再次阅读(inplace = True)
        self.conv3x3_up_2 = nn.Conv2d(in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1)自我。Conv3x3_up_2 = nn。Conv2d（in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1）
        self.batchnorm_up_2 = nn.BatchNorm2d(outchannel)自我。Batchnorm_up_2 = nn。BatchNorm2d (outchannel)
        self.relu_up_2 = nn.ReLU(inplace=True)自我。relu_up_2 = nn。再次阅读(inplace = True)

        #------------------------下面channel卷积-----------------------------------#
        self.conv3x3_down_1 = nn.Conv2d(in_channels=inchannel, out_channels=outchannel,kernel_size=3,stride=1,padding=1)自我。Conv3x3_down_1 = nn。Conv2d (in_channels = inchannel out_channels = outchannel kernel_size = 3,跨步= 1,填充= 1)
        self.batchnorm_down_1 = nn.BatchNorm2d(outchannel)自我。Batchnorm_down_1 = nn。BatchNorm2d (outchannel)
        self.relu_down_1 = nn.ReLU(inplace=True)自我。relu_down_1 = nn。再次阅读(inplace = True)
        self.conv3x3_down_2 = nn.Conv2d(in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1)自我。Conv3x3_down_2 = nn。Conv2d（in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1）
        self.batchnorm_down_2 = nn.BatchNorm2d(outchannel)自我。Batchnorm_down_2 = nn。BatchNorm2d (outchannel)
        self.relu_down_2 = nn.ReLU(inplace=True)自我。relu_down_2 = nn。再次阅读(inplace = True)

        #-------------------------上下channel结合卷积--------------------------------------------------#
        self.up_down_conv3x3_1 = nn.Conv2d(in_channels=outchannel*2,out_channels=outchannel,kernel_size=3,stride=1,padding=1)自我。Up_down_conv3x3_1 = nn。Conv2d (in_channels = outchannel * 2, out_channels = outchannel kernel_size = 3,跨步= 1,填充= 1)
        self.up_down_conv3x3_2 = nn.Conv2d(in_channels=outchannel,out_channels=outchannel,kernel_size=3,stride=1,padding=1)自我。Up_down_conv3x3_2 = nn。Conv2d (in_channels = outchannel out_channels = outchannel kernel_size = 3,跨步= 1,填充= 1)
        self.up_down_downsample = nn.Upsample(scale_factor=2,mode="bilinear",align_corners=True)自我。Up_down_downsample = nn。Upsample (scale_factor = 2,模式=“双线性align_corners = True)





    def forward(self,x,y):   def向前(自我,x, y):
        scale_channel_left = self.scale_channelscale_channel = self.scale_channel
        spx1 = torch.split(x,split_size_or_sections=scale_channel_left,dim=1)Spx1 = torch.split（x,split_size_or_sections=scale_channel_left,dim=1）
        channel_left_1, channel_left_2, channel_left_3, channel_left_4 = spx1Channel_left_1, channel_left_2, channel_left_3, channel_left_4 = spx1
        spx2 = torch.split(y, split_size_or_sections=scale_channel_left, dim=1)Spx2 =火炬。Split （y, split_size_or_sections=scale_channel_left, dim=1）
        channel_right_1, channel_right_2, channel_right_3, channel_right_4 = spx2Channel_right_1, channel_right_2, channel_right_3, channel_right_4 = spx2

        #-------------上面-------------------------#
        channel_left_1 = self.relu(self.batchnorm(self.conv3x3(channel_left_1)))Channel_left_1 = self.relu(self.batchnorm(self.conv3x3(Channel_left_1))))
        channel_left_1 = self.relu(self.batchnorm(self.conv3x3(channel_left_1)))Channel_left_1 = self.relu(self.batchnorm(self.conv3x3(Channel_left_1))))

        channel_left_2 = self.relu(self.batchnorm(self.conv3x3(channel_left_2)))Channel_left_2 = self.relu(self.batchnorm(self.conv3x3(Channel_left_2))))
        channel_left_2 = self.relu(self.batchnorm(self.conv3x3(channel_left_2)))Channel_left_2 = self.relu(self.batchnorm(self.conv3x3(Channel_left_2))))

        channel_right_1 = self.relu(self.batchnorm(self.conv3x3(channel_right_1)))Channel_right_1 = self.relu(self.batchnorm(self.conv3x3(Channel_right_1))))
        channel_right_1 = self.relu(self.batchnorm(self.conv3x3(channel_right_1)))Channel_right_1 = self.relu(self.batchnorm(self.conv3x3(Channel_right_1))))

        channel_right_2 = self.relu(self.batchnorm(self.conv3x3(channel_right_2)))Channel_right_2 = self.relu(self.batchnorm(self.conv3x3(Channel_right_2))))
        channel_right_2 = self.relu(self.batchnorm(self.conv3x3(channel_right_2)))Channel_right_2 = self.relu(self.batchnorm(self.conv3x3(Channel_right_2))))

        L1R1 = channel_left_1*channel_right_1
        L1R2 = channel_left_1*channel_right_2
        L2R1 = channel_left_2*channel_left_1
        L2R2 = channel_left_2*channel_right_2

        #--------------------上channel融合-------------------------------#
        UP = torch.cat((L1R1,L1R2,L2R1,L2R2),dim=1)UP = torch.cat（(L1R1,L1R2,L2R1,L2R2),dim=1）
        UP = self.relu_up_1(self.batchnorm_up_1(self.conv3x3_up_1(UP)))UP = self.relu_up_1(self.batchnorm_up_1(self.conv3x3_up_1(UP))))
        UP = self.relu_up_2(self.batchnorm_up_2(self.conv3x3_up_2(UP)))UP = self.relu_up_2（self.batchnorm_up_2(self.conv3x3_up_2(UP))）





        #------------下面------------------------------#
        channel_left_3 = self.relu(self.batchnorm(self.conv3x3(channel_left_3)))Channel_left_3 = self.relu(self.batchnorm(self.conv3x3(Channel_left_3))))
        channel_left_3 = self.relu(self.batchnorm(self.conv3x3(channel_left_3)))Channel_left_3 = self.relu(self.batchnorm(self.conv3x3(Channel_left_3))))

        channel_left_4 = self.relu(self.batchnorm(self.conv3x3(channel_left_4)))Channel_left_4 = self.relu(self.batchnorm(self.conv3x3(Channel_left_4))))
        channel_left_4 = self.relu(self.batchnorm(self.conv3x3(channel_left_4)))Channel_left_4 = self.relu(self.batchnorm(self.conv3x3(Channel_left_4))))

        channel_right_3 = self.relu(self.batchnorm(self.conv3x3(channel_right_3)))Channel_right_3 = self.relu(self.batchnorm(self.conv3x3(Channel_right_3))))
        channel_right_3 = self.relu(self.batchnorm(self.conv3x3(channel_right_3)))Channel_right_3 = self.relu(self.batchnorm(self.conv3x3(Channel_right_3))))

        channel_right_4 = self.relu(self.batchnorm(self.conv3x3(channel_right_4)))Channel_right_4 = self.relu(self.batchnorm(self.conv3x3(Channel_right_4))))
        channel_right_4 = self.relu(self.batchnorm(self.conv3x3(channel_right_4)))Channel_right_4 = self.relu(self.batchnorm(self.conv3x3(Channel_right_4))))

        L3R3 = channel_left_3 * channel_right_3
        L3R4 = channel_left_3 * channel_right_4
        L4R3 = channel_left_4 * channel_left_3
        L4R4 = channel_left_4 * channel_right_4

        # --------------------下channel融合-------------------------------#
        Down = torch.cat((L3R3,L3R4,L4R3,L4R4),dim=1)Down = torch.cat（(L3R3,L3R4,L4R3,L4R4),dim=1）
        Down = self.relu_down_1(self.batchnorm_down_1(self.conv3x3_down_1(Down)))Down = self.relu_down_1(self.batchnorm_down_1(self.conv3x3_down_1(Down))))
        Down = self.relu_down_2(self.batchnorm_down_2(self.conv3x3_down_2(Down)))Down = self.relu_down_2（self.batchnorm_down_2(self.conv3x3_down_2(Down))）


        #--------------------up_down通道信息融合--------------------------------------------#
        up_down = torch.cat((UP,Down),dim=1)
        up_down = self.up_down_conv3x3_1(up_down)
        up_down = self.up_down_conv3x3_2(up_down)
        up_down = self.up_down_downsample(up_down)





        return  up_down

#---------------------------------------Reverse feature extraction module-------------------------------------------------------------------------#
class RFEM(nn.Module):
    def __init__(self,inchannel,outchannel,scale=4):
        super(RFEM,self).__init__()
        self.scale_channel = int(inchannel/scale)
        #----------------------------------------上下channel公共卷积------------------------------------------------------#
        self.conv3x3 = nn.Conv2d(in_channels=self.scale_channel,out_channels=self.scale_channel,kernel_size=3,stride=1,padding=1)
        self.batchnorm = nn.BatchNorm2d(self.scale_channel)
        self.relu = nn.ReLU(inplace=True)

        #------------------------上面channel卷积---------------------------------#
        self.conv3x3_up_1 = nn.Conv2d(in_channels=inchannel,out_channels=outchannel,kernel_size=3,stride=1,padding=1)
        self.batchnorm_up_1 = nn.BatchNorm2d(outchannel)
        self.relu_up_1 = nn.ReLU(inplace=True)
        self.conv3x3_up_2 = nn.Conv2d(in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1)
        self.batchnorm_up_2 = nn.BatchNorm2d(outchannel)
        self.relu_up_2 = nn.ReLU(inplace=True)

        #------------------------下面channel卷积-----------------------------------#
        self.conv3x3_down_1 = nn.Conv2d(in_channels=inchannel, out_channels=outchannel,kernel_size=3,stride=1,padding=1)
        self.batchnorm_down_1 = nn.BatchNorm2d(outchannel)
        self.relu_down_1 = nn.ReLU(inplace=True)
        self.conv3x3_down_2 = nn.Conv2d(in_channels=outchannel, out_channels=outchannel, kernel_size=3, stride=1,padding=1)
        self.batchnorm_down_2 = nn.BatchNorm2d(outchannel)
        self.relu_down_2 = nn.ReLU(inplace=True)

        #-------------------------上下channel结合卷积--------------------------------------------------#
        self.up_down_conv3x3_1 = nn.Conv2d(in_channels=outchannel*2,out_channels=outchannel,kernel_size=3,stride=1,padding=1)
        self.up_down_conv3x3_2 = nn.Conv2d(in_channels=outchannel,out_channels=outchannel,kernel_size=3,stride=1,padding=1)
        self.up_down_downsample = nn.Upsample(scale_factor=2,mode="bilinear",align_corners=True)





    def forward(self,x,y):
        scale_channel_left = self.scale_channel
        spx1 = torch.split(x,split_size_or_sections=scale_channel_left,dim=1)
        channel_left_1, channel_left_2, channel_left_3, channel_left_4 = spx1
        spx2 = torch.split(y, split_size_or_sections=scale_channel_left, dim=1)
        channel_right_1, channel_right_2, channel_right_3, channel_right_4 = spx2

        #-------------上面-------------------------#
        channel_left_1 = self.relu(self.batchnorm(self.conv3x3(channel_left_1)))
        channel_left_1 = self.relu(self.batchnorm(self.conv3x3(channel_left_1)))

        channel_left_2 = self.relu(self.batchnorm(self.conv3x3(channel_left_2)))
        channel_left_2 = self.relu(self.batchnorm(self.conv3x3(channel_left_2)))

        channel_right_1 = self.relu(self.batchnorm(self.conv3x3(channel_right_1)))
        channel_right_1 = self.relu(self.batchnorm(self.conv3x3(channel_right_1)))

        channel_right_2 = self.relu(self.batchnorm(self.conv3x3(channel_right_2)))
        channel_right_2 = self.relu(self.batchnorm(self.conv3x3(channel_right_2)))

        L1R1 = channel_left_1*channel_right_1
        L1R2 = channel_left_1*channel_right_2
        L2R1 = channel_left_2*channel_left_1
        L2R2 = channel_left_2*channel_right_2

        #--------------------上channel融合-------------------------------#
        UP = torch.cat((L1R1,L1R2,L2R1,L2R2),dim=1)
        UP = self.relu_up_1(self.batchnorm_up_1(self.conv3x3_up_1(UP)))
        UP = self.relu_up_2(self.batchnorm_up_2(self.conv3x3_up_2(UP)))





        #------------下面------------------------------#
        channel_left_3 = self.relu(self.batchnorm(self.conv3x3(channel_left_3)))
        channel_left_3 = self.relu(self.batchnorm(self.conv3x3(channel_left_3)))

        channel_left_4 = self.relu(self.batchnorm(self.conv3x3(channel_left_4)))
        channel_left_4 = self.relu(self.batchnorm(self.conv3x3(channel_left_4)))

        channel_right_3 = self.relu(self.batchnorm(self.conv3x3(channel_right_3)))
        channel_right_3 = self.relu(self.batchnorm(self.conv3x3(channel_right_3)))

        channel_right_4 = self.relu(self.batchnorm(self.conv3x3(channel_right_4)))
        channel_right_4 = self.relu(self.batchnorm(self.conv3x3(channel_right_4)))

        L3R3 = channel_left_3 * channel_right_3
        L3R4 = channel_left_3 * channel_right_4
        L4R3 = channel_left_4 * channel_left_3
        L4R4 = channel_left_4 * channel_right_4

        # --------------------下channel融合-------------------------------#
        Down = torch.cat((L3R3,L3R4,L4R3,L4R4),dim=1)
        Down = self.relu_down_1(self.batchnorm_down_1(self.conv3x3_down_1(Down)))
        Down = self.relu_down_2(self.batchnorm_down_2(self.conv3x3_down_2(Down)))


        #--------------------up_down通道信息融合--------------------------------------------#
        up_down = torch.cat((UP,Down),dim=1)
        up_down = self.up_down_conv3x3_1(up_down)
        up_down = self.up_down_conv3x3_2(up_down)
        up_down = self.up_down_downsample(up_down)





        return  up_down


class channelsplite(nn.Module):   #将中间输出的图片，经过rcl卷积，将mask进行输入，在进行bhs模型损失
    def __init__(self,inchannel,outchannel,scale=4):
        super(channelsplite,self).__init__()
        self.scale_channel = int(inchannel / scale)
#--------------------------------------------卷积核------------------------------------------------------------------------------------------#
        self.conv_1x1 = nn.Conv2d(in_channels=self.scale_channel,out_channels=self.scale_channel,kernel_size=1,stride=1,padding=0)
        self.conv_3x3 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=3, stride=1, padding=1)
        self.conv_5x5 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=5, stride=1, padding=2)
        self.conv_7x7 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=7, stride=1, padding=3)

    def forward(self, x):
        scale_channel_left = self.scale_channel
        spx1 = torch.split(x, split_size_or_sections=scale_channel_left, dim=1)
        channel_1, channel_2, channel_3, channel_4 = spx1
        channel_1 = self.conv_1x1(channel_1)
        channel_1_2 = channel_1
        channel_2 = self.conv_3x3(channel_2 + channel_1_2)
        channel_2_2 = channel_2
        channel_3 = self.conv_5x5(channel_3 + channel_2_2)
        channel_3_2 = channel_3
        channel_4 = self.conv_7x7(channel_4 + channel_3_2)
        x1 = torch.cat((channel_1, channel_2, channel_3, channel_4), dim=1)  # 1,64,224,224

        return x1

class channelsplite_large_kernel(nn.Module):   #将中间输出的图片，经过rcl卷积，将mask进行输入，在进行bhs模型损失
    def __init__(self,inchannel,scale=4):
        super(channelsplite_large_kernel,self).__init__()
        self.scale_channel = int(inchannel / scale)
#--------------------------------------------卷积核------------------------------------------------------------------------------------------#
        self.conv_1x1 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=1,
                                  stride=1, padding=0)
        self.conv_3x3 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=3,
                                  stride=1, padding=1)
        self.conv_5x5 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=5,
                                  stride=1, padding=2)
        self.conv_7x7 = nn.Conv2d(in_channels=self.scale_channel, out_channels=self.scale_channel, kernel_size=7,
                                  stride=1, padding=3)

    def forward(self, x):
        scale_channel_left = self.scale_channel
        spx1 = torch.split(x, split_size_or_sections=scale_channel_left, dim=1)
        channel_1, channel_2, channel_3, channel_4 = spx1
        channel_1 = self.conv_1x1(channel_1)
        channel_1_2 = channel_1

        channel_2 = self.conv_3x3(channel_2 + channel_1_2)
        channel_2_2 = channel_2

        channel_3 = self.conv_3x3(channel_3 + channel_2_2)
        channel_3 = self.conv_5x5(channel_3)
        channel_3_2 = channel_3

        channel_4 = self.conv_3x3(channel_4 + channel_3_2)
        channel_4 = self.conv_5x5(channel_4)
        channel_4 = self.conv_7x7(channel_4)

        x1 = torch.cat((channel_1, channel_2, channel_3, channel_4), dim=1)  # 1,64,224,224

        return x1


class RFM(nn.Module):   #Refined Module
    def __init__(self,inchannnels,outchannnels):
        super(RFM,self).__init__()
        self.channelsplite = channelsplite(64,64)
        self.channelsplite2 = channelsplite(128, 128)
        self.channelsplite3 = channelsplite(256, 256)
        self.channelsplite4 = channelsplite(512, 512)
        self.channelsplite5 = channelsplite(1024, 1024)

        self.channelsplite_dialation = channelsplite_large_kernel(64,4)

        self.upsample = nn.Upsample(scale_factor=2,mode="bilinear",align_corners=True)
        self.downsample = nn.MaxPool2d(2,2,ceil_mode=True)
        self.conv00 = nn.Conv2d(in_channels=inchannnels,out_channels=64,kernel_size=3,stride=1,padding=1)
        self.conv01 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv02 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv03 = nn.Conv2d(in_channels=64,out_channels=outchannnels,kernel_size=3,stride=1,padding=1)
        self.conv04 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)
        self.conv05 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=0)
        self.conv06 = nn.Conv2d(in_channels=inchannnels, out_channels=64, kernel_size=1, stride=1, padding=0)

        self.bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)









    def forward(self,x):
        x0 = self.conv00(x)

        #1,64,224,224
        #bolck
        x1 = self.relu(self.bn(self.conv01(x0)))
        x1 = self.channelsplite(x1)    #1,64,224,224
        x1 = self.relu(self.bn(self.conv01(x1)))

        x2 = self.downsample(x1)

        x2 = self.relu(self.bn(self.conv01(x2)))
        x2 = self.channelsplite(x2)
        x2 = self.relu(self.bn(self.conv01(x2)))

        x3 = self.downsample(x2)

        x3 = self.relu(self.bn(self.conv01(x3)))
        x3 = self.channelsplite(x3)
        x3 = self.relu(self.bn(self.conv01(x3)))

        x4 = self.downsample(x3)

        x4 = self.relu(self.bn(self.conv01(x4)))
        x4 = self.channelsplite(x4)  # 28
        x4 = self.relu(self.bn(self.conv01(x4)))

        x5 = self.downsample(x4)

        x5 = self.relu(self.bn(self.conv01(x5))) #14
        x5 = self.channelsplite_dialation(x5)
        x5 = self.relu(self.bn(self.conv01(x5)))

        x6 = self.upsample(x5)
        x6 = torch.cat((x6,x4),dim=1)   #1,128,28,28

        x6 = self.relu(self.bn(self.conv02(x6)))
        x6 = self.channelsplite(x6)
        x6 = self.relu(self.bn(self.conv01(x6)))#64,28,28

        x7 = self.upsample(x6)
        x7 = torch.cat((x7, x3), dim=1)  # 1,128,28,28

        x7 = self.relu(self.bn(self.conv02(x7)))
        x7 = self.channelsplite(x7)
        x7 = self.relu(self.bn(self.conv01(x7)))# 64,56,56

        x8 = self.upsample(x7)
        x8 = torch.cat((x8, x2), dim=1)  # 1,128,112,112

        x8 = self.relu(self.bn(self.conv02(x8)))
        x8 = self.channelsplite(x8)
        x8 = self.relu(self.bn(self.conv01(x8)))# 64,112,112

        x9 = self.upsample(x8)
        x9 = torch.cat((x9, x1), dim=1)  # 1,128,224,224

        x9 = self.relu(self.bn(self.conv02(x9)))
        x9 = self.channelsplite(x9)
        x9 = self.relu(self.bn(self.conv01(x9)))# 64,224,224


        CSM_out = self.conv03(x9)







        # return CSM_out
        return CSM_out +x


class ChannelAttentionModule(nn.Module):
    def __init__(self,channel):
        super(ChannelAttentionModule, self).__init__()

        self.channel_attention_avg = nn.Sequential(
            # nn.AvgPool2d((height, width)),  # 参数设置为输入特征图的高度和宽度，即256和256。这意味着对于每个通道，所有256x256的像素值将计算平均值，结果是每个通道只剩下一个单一的平均值，
            nn.AdaptiveAvgPool2d(1),
            nn.Softmax(dim=1)
        )
        self.channel_attention_max = nn.Sequential(
            nn.AdaptiveMaxPool2d(1),
            nn.Softmax(dim=1)
        )
        self.shared_MLP = nn.Sequential(
            nn.Linear(in_features=channel, out_features=int(channel / 2)),
            nn.ReLU(),
            nn.Linear(in_features=int(channel / 2), out_features=channel)
        )
        self.linear_1 = nn.Linear(in_features=channel,out_features=int(channel/2))
        self.linear_2 = nn.Linear(in_features=int(channel/2),out_features=channel)
        self.relu = nn.ReLU()

    def forward(self, x):
        x_avg_out = self.channel_attention_avg(x)      #1,64,256,256----1,64,1,1
        x_avg = x_avg_out.view(x_avg_out.size(0), -1)   #1,64
        x_avg = self.relu(self.linear_1(x_avg))    #1,32
        x_avg = self.linear_2(x_avg)        #1,64
        x_avg = x_avg.view(x_avg_out.size(0),x_avg_out.size(1),x_avg_out.size(2),x_avg_out.size(3))

        x_max_out = self.channel_attention_max(x)    #1,64,1,1
        x_max = x_max_out.view(x_max_out.size(0),-1)
        x_max = self.relu(self.linear_1(x_max))
        x_max = self.linear_2(x_max)
        x_max = x_max.view(x_max_out.size(0),x_max_out.size(1),x_max_out.size(2),x_max_out.size(3)) #1,64,1,1

        return  x_avg,x_max

class SpatialAttentionModule(nn.Module):
    def __init__(self,):
        super(SpatialAttentionModule, self).__init__()

        self.spatial_attention = nn.Softmax(dim=2)

    def forward(self, x):
        x_spa =x.view([x.size(0), x.size(1), -1])   #1,64,512x512
        x_spa = self.spatial_attention(x_spa)
        x_spa = x_spa.view(x.size(0),x.size(1),x.size(2),x.size(3))

        return  x_spa

class CBPAM(nn.Module):      #CHANNEL SPATIONAL PARALLEL  ATTENTION MODULE
    def __init__(self,channel):
        super(CBPAM, self).__init__()
        self.ChannelAttentionModule = ChannelAttentionModule(channel)
        self.SpatialAttentionModule = SpatialAttentionModule()
        self.signoid = nn.Sigmoid()

    def forward(self,x):
        x_avg,x_max = self.ChannelAttentionModule(x)
        x_spa = self.SpatialAttentionModule(x)

        x_out =self.signoid(x_avg*x_spa+x_avg*x_max)

        return x_out*x


class RFLN(nn.Module):   #将模型中的3x3,csm,3x3结构变换成 1x1，csm,1x1结构
    def __init__(self,n_channels,n_classes):
        super(RFLN,self).__init__()

        resnet = models.resnet34(weights=ResNet34_Weights.DEFAULT)   #使用预训练的resnet34来初始化编译器器部分
        self.CBPAM5 =CBPAM(512)
        self.CBPAM4 =CBPAM(512)
        self.CBPAM3 =CBPAM(256)
        self.CBPAM2 =CBPAM(128)
        self.CBPAM1 =CBPAM(64)



        ## -------------Encoder--------------

        self.inconv = nn.Conv2d(n_channels,64,3,padding=1)
        self.inbn = nn.BatchNorm2d(64)
        self.inrelu = nn.ReLU(inplace=True)

        #stage 1
        self.encoder1 = resnet.layer1 #224
        #stage 2
        self.encoder2 = resnet.layer2 #112
        #stage 3
        self.encoder3 = resnet.layer3 #56
        #stage 4
        self.encoder4 = resnet.layer4 #28

        self.pool4 = nn.MaxPool2d(2,2,ceil_mode=True)

        #stage 5
        self.resb5_1 = BasicBlock(512,512)
        self.resb5_2 = BasicBlock(512,512)
        self.resb5_3 = BasicBlock(512,512) #14

        self.pool5 = nn.MaxPool2d(2,2,ceil_mode=True)

        #stage 6
        self.resb6_1 = BasicBlock(512,512)
        self.resb6_2 = BasicBlock(512,512)
        self.resb6_3 = BasicBlock(512,512) #7

        ## -------------Bridge--------------

        #stage Bridge
        self.convbg_1 = nn.Conv2d(512,512,3,dilation=2, padding=2) # 7
        self.bnbg_1 = nn.BatchNorm2d(512)
        self.relubg_1 = nn.ReLU(inplace=True)
        self.convbg_m = nn.Conv2d(512,512,3,dilation=2, padding=2)
        self.bnbg_m = nn.BatchNorm2d(512)
        self.relubg_m = nn.ReLU(inplace=True)
        self.convbg_2 = nn.Conv2d(512,512,3,dilation=2, padding=2)
        self.bnbg_2 = nn.BatchNorm2d(512)
        self.relubg_2 = nn.ReLU(inplace=True)

        ## -------------Decoder--------------

        #stage 6d
        self.conv6d_1 = nn.Conv2d(1024,512,3,padding=1) # 16
        self.bn6d_1 = nn.BatchNorm2d(512)
        self.relu6d_1 = nn.ReLU(inplace=True)

        self.conv6d_m = nn.Conv2d(512,512,3,dilation=2, padding=2)###
        self.bn6d_m = nn.BatchNorm2d(512)
        self.relu6d_m = nn.ReLU(inplace=True)

        self.conv6d_2 = nn.Conv2d(512,512,3,dilation=2, padding=2)
        self.bn6d_2 = nn.BatchNorm2d(512)
        self.relu6d_2 = nn.ReLU(inplace=True)

        #stage 5d
        self.conv5d_1 = nn.Conv2d(1024,512,3,padding=1) # 16
        self.bn5d_1 = nn.BatchNorm2d(512)
        self.relu5d_1 = nn.ReLU(inplace=True)

        self.conv5d_m = nn.Conv2d(512,512,3,padding=1)###
        self.bn5d_m = nn.BatchNorm2d(512)
        self.relu5d_m = nn.ReLU(inplace=True)

        self.conv5d_2 = nn.Conv2d(512,512,3,padding=1)
        self.bn5d_2 = nn.BatchNorm2d(512)
        self.relu5d_2 = nn.ReLU(inplace=True)

        #stage 4d
        self.conv4d_1 = nn.Conv2d(1024,512,3,padding=1) # 32
        self.bn4d_1 = nn.BatchNorm2d(512)
        self.relu4d_1 = nn.ReLU(inplace=True)

        self.conv4d_m = nn.Conv2d(512,512,3,padding=1)###
        self.bn4d_m = nn.BatchNorm2d(512)
        self.relu4d_m = nn.ReLU(inplace=True)

        self.conv4d_2 = nn.Conv2d(512,256,3,padding=1)
        self.bn4d_2 = nn.BatchNorm2d(256)
        self.relu4d_2 = nn.ReLU(inplace=True)

        #stage 3d
        self.conv3d_1 = nn.Conv2d(512,256,3,padding=1) # 64
        self.bn3d_1 = nn.BatchNorm2d(256)
        self.relu3d_1 = nn.ReLU(inplace=True)

        self.conv3d_m = nn.Conv2d(256,256,3,padding=1)###
        self.bn3d_m = nn.BatchNorm2d(256)
        self.relu3d_m = nn.ReLU(inplace=True)

        self.conv3d_2 = nn.Conv2d(256,128,3,padding=1)
        self.bn3d_2 = nn.BatchNorm2d(128)
        self.relu3d_2 = nn.ReLU(inplace=True)

        #stage 2d

        self.conv2d_1 = nn.Conv2d(256,128,3,padding=1) # 128
        self.bn2d_1 = nn.BatchNorm2d(128)
        self.relu2d_1 = nn.ReLU(inplace=True)

        self.conv2d_m = nn.Conv2d(128,128,3,padding=1)###
        self.bn2d_m = nn.BatchNorm2d(128)
        self.relu2d_m = nn.ReLU(inplace=True)

        self.conv2d_2 = nn.Conv2d(128,64,3,padding=1)
        self.bn2d_2 = nn.BatchNorm2d(64)
        self.relu2d_2 = nn.ReLU(inplace=True)

        #stage 1d
        self.conv1d_1 = nn.Conv2d(128,64,3,padding=1) # 256
        self.bn1d_1 = nn.BatchNorm2d(64)
        self.relu1d_1 = nn.ReLU(inplace=True)

        self.conv1d_m = nn.Conv2d(64,64,3,padding=1)###
        self.bn1d_m = nn.BatchNorm2d(64)
        self.relu1d_m = nn.ReLU(inplace=True)

        self.conv1d_2 = nn.Conv2d(64,64,3,padding=1)
        self.bn1d_2 = nn.BatchNorm2d(64)
        self.relu1d_2 = nn.ReLU(inplace=True)

        ## -------------Bilinear Upsampling--------------
        self.upscore6 = nn.Upsample(scale_factor=32,mode='bilinear')###
        self.upscore5 = nn.Upsample(scale_factor=16,mode='bilinear')
        self.upscore4 = nn.Upsample(scale_factor=8,mode='bilinear')
        self.upscore3 = nn.Upsample(scale_factor=4,mode='bilinear')
        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear')

        ## -------------Side Output--------------
        self.outconvb = nn.Conv2d(512, 1, 3, padding=1)
        self.outconv6 = nn.Conv2d(512, 1, 3, padding=1)
        self.outconv5 = nn.Conv2d(512, 1, 3, padding=1)
        self.outconv4 = nn.Conv2d(256, 1, 3, padding=1)
        self.outconv3 = nn.Conv2d(128, 1, 3, padding=1)
        self.outconv2 = nn.Conv2d(64, 1, 3, padding=1)
        self.outconv_1 = nn.Conv2d(64, 1, 3, padding=1)
        #------------------直接输出------------------------#
        self.outconv_x4 = nn.Conv2d(64,1,1,stride=1,padding=0)
        self.outconv_x3 = nn.Conv2d(128, 1, 1, stride=1, padding=0)
        self.outconv_x2 = nn.Conv2d(256, 1, 1, stride=1, padding=0)
        self.outconv_x1 = nn.Conv2d(512, 1, 1, stride=1, padding=0)
        self.outconv_x0 = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,stride=1,padding=1)

        ## -------------Refine Module-------------
        self.RFM = RFM(1,1)

        #-----------------通道融合-----------------------------------#
        self.stage1 = channelmix(512,256)自我。Stage1 = channelmix（512,256）
        self.stage2 = channelmix(256, 128)
        self.stage3 = channelmix(128, 64)
        self.stage4 = channelmix(64, 1)






    def forward(self,x):

        hx = x   Hx = x

        ## -------------Encoder-------------
        hx = self.inconv(hx)
        hx = self.inbn(hx)
        hx = self.inrelu(hx)

        h1 = self.encoder1(hx) # 256
        h2 = self.encoder2(h1) # 128
        h3 = self.encoder3(h2) # 64
        h4 = self.encoder4(h3) # 32

        hx = self.pool4(h4) # 16

        hx = self.resb5_1(hx)
        hx = self.resb5_2(hx)
        h5 = self.resb5_3(hx)

        hx = self.pool5(h5) # 8

        hx = self.resb6_1(hx)
        hx = self.resb6_2(hx)
        h6 = self.resb6_3(hx)


        ## -------------Bridge-------------
        hx = self.relubg_1(self.bnbg_1(self.convbg_1(h6)))
        hx = self.relubg_m(self.bnbg_m(self.convbg_m(hx)))Hx = self.relubg_m（self.bnbg_m(self.convbg_m(Hx))）
        hbg = self.relubg_2(self.bnbg_2(self.convbg_2(hx))) #1,512,7,7HBG = self.relubg_2（self.bnbg_2(self. bnbg_2)）convbg_2 (hx))) # 1512、7、7

        ## -------------Decoder-------------## ------------- 译码器 -------------

        hx = self.relu6d_1(self.bn6d_1(self.conv6d_1(torch.cat((hbg,h6),1))))
        hx = self.relu6d_m(self.bn6d_m(self.conv6d_m(hx)))
        hd6 = self.relu6d_2(self.bn6d_2(self.conv6d_2(hx))) #1,512,7,7
        hd6_upsample = self.upscore2(hd6)   #1,512,14,14

        hx = self.upscore2(hd6) # 8 -> 16

        hx = self.relu5d_1(self.bn5d_1(self.conv5d_1(torch.cat((hx,self.CBPAM5(h5)),1))))
        hx = self.relu5d_m(self.bn5d_m(self.conv5d_m(hx)))
        hd5 = self.relu5d_2(self.bn5d_2(self.conv5d_2(hx))) #1,512,14,14

        hx = self.upscore2(hd5) # 16 -> 32

        hx = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((hx,self.CBPAM4(h4)),1))))
        hx = self.relu4d_m(self.bn4d_m(self.conv4d_m(hx)))
        hd4 = self.relu4d_2(self.bn4d_2(self.conv4d_2(hx))) #1,256,28,28

        hx = self.upscore2(hd4) # 32 -> 64

        hx = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((hx,self.CBPAM3(h3)),1))))hx = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((hx,self.CBPAM3(h3))),1))))
        hx = self.relu3d_m(self.bn3d_m(self.conv3d_m(hx)))Hx = self.relu3d_m（self.bn3d_m(self.conv3d_m(Hx))）
        hd3 = self.relu3d_2(self.bn3d_2(self.conv3d_2(hx))) #1,128,56,56Hd3 = self.relu3d_2（self.bn3d_2）conv3d_2 (hx))) # 1128, 56   𝐶岁,56   𝐶

        hx = self.upscore2(hd3) # 64 -> 128Hx =自我。Upscore2 (hd3) # 64 ->

        hx = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((hx,self.CBPAM2(h2)),1))))hx = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((hx,self.CBPAM2(h2))),1))))
        hx = self.relu2d_m(self.bn2d_m(self.conv2d_m(hx)))Hx = self.relu2d_m（self.bn2d_m(self.conv2d_m(Hx))）
        hd2 = self.relu2d_2(self.bn2d_2(self.conv2d_2(hx))) #1,64,112,122Hd2 = self.relu2d_2（self.bn2d_2）conv2d_2 (hx))) # 1, 64112122

        hx = self.upscore2(hd2) # 128 -> 256Hx =自我。Upscore2 (hd2) # 128 -> 256

        hx = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((hx,self.CBPAM1(h1)),1))))hx = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((hx,self.CBPAM1(h1))),1)))) .
        hx = self.relu1d_m(self.bn1d_m(self.conv1d_m(hx)))Hx = self.relu1d_m（self.bn1d_m(self.conv1d_m(Hx))）
        hd1 = self.relu1d_2(self.bn1d_2(self.conv1d_2(hx))) #1,64,224,224Hd1 = self.relu1d_2（self.bn1d_2）conv1d_2 (hx))) # 1, 64224224


        # ------------------层信息------------------------------------#
        #hbg 1,512,7,7
        #hd6 1,512,7,7   # hd6 1512、7、7
        #hd5 1,512,14,14   14 # hd5 1512年,14岁
        #hd4 1,256,28,28   28 # hd4 1256年28日
        #hd3 1,128,56,56   56   𝐶岁的# hd3 1128 56   𝐶
        #hd2 1,64,112,112   # hd2 1, 64112112


        ## -------------Side Output-------------## -------------侧输出-------------
        # db = self.outconvb(hbg)#1,1,7,7
        # d6 = self.outconv6(hd6)#1,1,7,7
        d5 = self.outconv5(hd5)#1,1,14,14D5 = self.outconv5(hd5)#1,1,14,14
        d4 = self.outconv4(hd4)#1,1,28,28D4 = self.outconv4(hd4)#1,1,28,28
        d3 = self.outconv3(hd3)#1,1,56,56D3 = self.outconv3(hd3)#1,1,56   𝐶,56   𝐶
        d2 = self.outconv2(hd2)#1,1,112,112D2 = self.outconv2(hd2)#1,1,112,112
        d1 = self.outconv_1(hd1)   #1,1,224,224,D1 =自我。1224224年outconv_1 (hd1) # 1,

        #-----------------Reverse feature localization branch module----------------------------##----------------- 反向功能定位分支模块 ----------------------------#
        stage1 = self.stage1(hd6_upsample,hd5)  #512Stage1 =自我。stage1 (hd6_upsample hd5) # 512   𝐱′
        stage2 = self.stage2(stage1,hd4)    #256阶段2 =自我。stage2——hd4 stage1 # 256
        stage3 = self.stage3(stage2,hd3)       #128阶段3 = self。stage3——stage2, hd3 # 128
        stage4 = self.stage4(stage3,hd2)    #1,1,224,224阶段4 = self。stage4——hd2 stage3 1,1,224,224 #





        ## -------------Refine Module-------------## -------------细化模块-------------
        dout_cms = self.RFM(d1) #224Dout_cms = self。RFM (d1) # 224







        return  F.sigmoid(dout_cms),F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5),F.sigmoid(stage4)返回F.sigmoid (dout_cms), F。sigmoid(d1)， F.sigmoid(d2)， F.sigmoid(d3)， F.sigmoid(d4)， F.sigmoid(d5)，F.sigmoid（stage4）

        # return F.sigmoid(dout_cms)# return   返回 F.sigmoid（dout_cms）
        #返回的是8的tuple数据类型的值 len(return)


if __name__ == "__main__":   如果__name__ == "__main__":
    x = torch.randn((1, 3, 288,288))X =火炬。Randn （(1,3,288,288)）
    # y = torch.randn((1, 64, 288, 288))# y =火炬。Randn （(1,64,288,288)）
    model=RFLN(3,1)   模型= RFLN (3,1)
    # module = CBPAM(64)   # module = CBPAM（64）
    output =model(x)   输出(x) =模型
    print("=", output[0].size())打印(“=”,输出[0].size ())
    # model = RFLN(n_channels=3, n_classes=1)# model   模型 = RFLN（n_channels=3, n_classes=1）

    # 计算总参数数量和可训练参数数量
    total_params = sum(p.numel() for p in model.parameters())Total_params = sum(p。model   模型.parameters（）中p的Numel （）
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)Trainable_params = sum(p。如果p.requires_grad，则model   模型.parameters（）中的p为Numel （）

    print(f"Total parameters: {total_params}")print（f“总参数：{total_params}”）
    print(f"Trainable parameters: {trainable_params}")print（f“可训练参数：{trainable_params}”）


